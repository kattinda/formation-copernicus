{
  "metadata": {
    "name": "Formation ML0 - Prise en main",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Manipulation de Spark Scala"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// variable immuable\nval x\u003d1"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nx\u003d2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprintln(x)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// variable muable\nvar y\u003d2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprintln(y)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ny\u003d3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprintln(y)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "L\u0027objectif est de prendre en main les fonctions basiques de Spark Scala.\n\n### Read dataset (csv format) from HDFS\n\nHere we use the dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n\nThe target variable will be class (there are 3 classes : Iris Setosa, Iris Versicolour, Iris Virginica) and the variables descriptives are : \n- sepal length\n- sepal width\n- petal length\n- petal width"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql._            \nval spark \u003d SparkSession.builder().getOrCreate()\n\nval df \u003d spark.read.format(\"com.databricks.spark.csv\")\n              .option(\"header\", \"true\")\n              .option(\"inferSchema\", \"true\") \n              .load(\"/opt/hupi/DATA/UPPA_2022/Data/clustering/iris.csv\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Show the dataframe\nUse the function `show`"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "###  Print the schema of the dataframe\nUse the fonction `printSchema`"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Select some columns\nUse the function `select`"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Rename some columns\nUse the fonction `withColumnRenamed(\"actual_name\", \"new_name\")`\nFor example rename the column \"class\" to \"irisClass\"."
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Get the distinct values of a columns\nUse the function `select`and the function `distinct`"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Insert a new column\nUse the fonction `withColumn`\nFor example add a new column with the categorisation of the Iris classes into numeric variable.\nThe column will be `IrisNum` and we want the following classification :\n- Iris-virginica \u003d\u003e 0\n- Iris-setosa \u003d\u003e 1\n- Iris-versicolor \u003d\u003e 2\n\nYou will need to use operators `when` and `otherwise`."
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Count the number of lines of a dataframe\nUse the function `count`"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Change the type of column\nThe transformations on some columns are made for example doing a `withColumn`.\nAnd inside the `withColumn` you can make some transformation using some created functions or existing functions.\n\n**For example**: Use the function `cast` in order to change the type of the column \"sepalLength\" to string type."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Define a function"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Define a simple function to print a greeting\ndef sayHello(name: String): Unit \u003d {\n  println(s\"Hello, $name!\")\n}\n\n// Call the function\nsayHello(\"John\")\n\n// Output\n// Hello, John!"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Exercice d\u0027application\n\nLe jeu de données Titanic fait référence au naufrage du fameux paquebot le Titanic en 1912, où il y eut beaucoup de noyés à cause du nombre insuffisant de canots de sauvetage.\nIl contient pour chaque passager les informations suivantes :\n\n- PassengerId : identifiant d’un passager.\n- Survived : Survie (0 \u003d Non; 1 \u003d Oui)\n- Pclass : Classe (1, 2 ou 3) indiquant la classe de la cabine, qui correspondent respectivement à la première, seconde et troisième classe.\n- Nom\n- Sexe\n- Age : variable indiquant l’âge du passager\n- SibSp : Nombre de frères et soeurs / épouses à bord\n- Parch : Nombre de parents / enfants à bord\n- Fare : Tarif pour le passager\n- Embarked : Port d\u0027embarquement (Cherbourg, Queenstown ou Southampton)\n\nAprès retrait des lignes avec valeurs manquantes, le tableau analysé contient 1045 personnes."
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql._            \nval spark \u003d SparkSession.builder().getOrCreate()\n\nval df \u003d spark.read.format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\")\n                .option(\"inferSchema\", \"true\") \n                .load(\"hdfs://ecoles-projets.node1.pro.hupi.loc:8020/user/ecoles-projets/Data_spark/titanic.csv\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Show the dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "###  Print the schema of the dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Select distinct values of a column\nDisplay the different class contained in `Embarked` column (select and distinct)."
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndata.select(\"Embarked\").distinct().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter the DataFrame based on a condition\nFilter the observtions with a Embarked column with null values."
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.filter(col(\"Embarked\").isNull).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter the DataFrame based on a condition\nFilter the observtions with a Embarked column with not null values."
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.filter(col(\"Embarked\").isNotNull).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter the DataFrame based on a condition\nFilter the observtions with a Fare greater or equal than 10$"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.filter(col(\"Fare\") \u003e\u003d 10).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Rename some columns\nUse the fonction `withColumnRenamed(\"actual_name\", \"new_name\")`\nRename the column \"PassengerId\" to \"Id\"."
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\ndf.withColumnRenamed(\"PassengerId\", \"Id\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Count the number of observations in the dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Drop rows with na and count the number of remaining observations"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.na.drop().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}