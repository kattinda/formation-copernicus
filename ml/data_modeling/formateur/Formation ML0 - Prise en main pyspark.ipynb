{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulation de Spark Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// variable immuable\n",
    "val x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "x=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "println(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// variable muable\n",
    "var y=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "println(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "y=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "println(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de prendre en main les fonctions basiques de Spark Scala.\n",
    "\n",
    "### Read dataset (csv format) from HDFS\n",
    "\n",
    "Here we use the dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "\n",
    "The target variable will be class (there are 3 classes : Iris Setosa, Iris Versicolour, Iris Virginica) and the variables descriptives are : \n",
    "- sepal length\n",
    "- sepal width\n",
    "- petal length\n",
    "- petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.sql._            \n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val df = spark.read.format(\"com.databricks.spark.csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\") \n",
    "              .load(\"/opt/hupi/DATA/UPPA_2022/Data/clustering/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the dataframe\n",
    "Use the function `show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Print the schema of the dataframe\n",
    "Use the fonction `printSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select some columns\n",
    "Use the function `select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename some columns\n",
    "Use the fonction `withColumnRenamed(\"actual_name\", \"new_name\")`\n",
    "For example rename the column \"class\" to \"irisClass\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the distinct values of a columns\n",
    "Use the function `select`and the function `distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert a new column\n",
    "Use the fonction `withColumn`\n",
    "For example add a new column with the categorisation of the Iris classes into numeric variable.\n",
    "The column will be `IrisNum` and we want the following classification :\n",
    "- Iris-virginica => 0\n",
    "- Iris-setosa => 1\n",
    "- Iris-versicolor => 2\n",
    "\n",
    "You will need to use operators `when` and `otherwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of lines of a dataframe\n",
    "Use the function `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the type of column\n",
    "The transformations on some columns are made for example doing a `withColumn`.\n",
    "And inside the `withColumn` you can make some transformation using some created functions or existing functions.\n",
    "\n",
    "**For example**: Use the function `cast` in order to change the type of the column \"sepalLength\" to string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "// Define a simple function to print a greeting\n",
    "def sayHello(name: String): Unit = {\n",
    "  println(s\"Hello, $name!\")\n",
    "}\n",
    "\n",
    "// Call the function\n",
    "sayHello(\"John\")\n",
    "\n",
    "// Output\n",
    "// Hello, John!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice d'application\n",
    "\n",
    "Le jeu de données Titanic fait référence au naufrage du fameux paquebot le Titanic en 1912, où il y eut beaucoup de noyés à cause du nombre insuffisant de canots de sauvetage.\n",
    "Il contient pour chaque passager les informations suivantes :\n",
    "\n",
    "- PassengerId : identifiant d’un passager.\n",
    "- Survived : Survie (0 = Non; 1 = Oui)\n",
    "- Pclass : Classe (1, 2 ou 3) indiquant la classe de la cabine, qui correspondent respectivement à la première, seconde et troisième classe.\n",
    "- Nom\n",
    "- Sexe\n",
    "- Age : variable indiquant l’âge du passager\n",
    "- SibSp : Nombre de frères et soeurs / épouses à bord\n",
    "- Parch : Nombre de parents / enfants à bord\n",
    "- Fare : Tarif pour le passager\n",
    "- Embarked : Port d'embarquement (Cherbourg, Queenstown ou Southampton)\n",
    "\n",
    "Après retrait des lignes avec valeurs manquantes, le tableau analysé contient 1045 personnes.\n",
    "\n",
    "1. Show the dataframe\n",
    "2. Print the schema of the dataframe\n",
    "3. Select the columns `Survived` and `Name`\n",
    "4. Rename the column `PassengerId` to `Id`\n",
    "5. Get the distinct values of the column `Embarked`\n",
    "6. Count how many NaN values in the column `Embarked`\n",
    "7. Filter the rows with NaN values in the column `Embarked`\n",
    "8. Drop all rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "import org.apache.spark.sql._            \n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val df = spark.read.format(\"com.databricks.spark.csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\") \n",
    "                .load(\"hdfs://ecoles-projets.node1.pro.hupi.loc:8020/user/ecoles-projets/Data_spark/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Print the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select distinct values of a column\n",
    "Display the different class contained in `Embarked` column (select and distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "data.select(\"Embarked\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame based on a condition\n",
    "Filter the observtions with a Embarked column with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.filter(col(\"Embarked\").isNull).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame based on a condition\n",
    "Filter the observtions with a Embarked column with not null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.filter(col(\"Embarked\").isNotNull).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame based on a condition\n",
    "Filter the observtions with a Fare greater or equal than 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.filter(col(\"Fare\") >= 10).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename some columns\n",
    "Use the fonction `withColumnRenamed(\"actual_name\", \"new_name\")`\n",
    "Rename the column \"PassengerId\" to \"Id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.withColumnRenamed(\"PassengerId\", \"Id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of observations in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with na and count the number of remaining observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n",
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "name": "Formation ML0 - Prise en main"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
