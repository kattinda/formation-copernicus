{
  "metadata": {
    "name": "Formation ML1 - Regression",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this section, let us present to you some Machine Learning algorithms, there are many, but 3 algorithms below can be considered as the most popular in Machine Learning :\n\n- 1/ Regression - Linear Regression\n- 2/ Classification - Random Forest\n- 3/ Clustering - KMeans\n\nThis notebook will focus on the first one, we\u0027ll take a dataset and then build a linear regression model based on it. \n\n\"Linear regression is the most basic type of regression and commonly used predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome variable?  Is the model using the predictors accounting for the variability in the changes in the dependent variable? (2) Which variables in particular are significant predictors of the dependent variable?  And in what way do they--indicated by the magnitude and sign of the beta estimates--impact the dependent variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. (3) What is the regression equation that shows how the set of predictor variables can be used to predict the outcome?  The simplest form of the equation with one dependent and one independent variable is defined by the formula y \u003d c + b*x, where y \u003d estimated dependent score, c \u003d constant, b \u003d regression coefficients, and x \u003d independent variable.\"\n\n(source : http://www.statisticssolutions.com/what-is-linear-regression/)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Read dataset (csv format) from HDFS\n\nHere we use the dataset from https://gist.github.com/seankross/a412dfbd88b3db70b74b#file-mtcars-csv\n\nLes données ont été extraites du magazine américain Motor Trend de 1974 et comprennent la consommation de carburant et 10 aspects de la conception et des performances automobiles pour 32 automobiles (modèles 1973-1974).\n\nUne base de données avec 32 observations sur 11 variables (numériques).\n\n[, 1]\tmpg\tMiles/gallon (US)\n[, 2]\tcylindre\tNombre de cylindres\n[, 3]\tafficher\tDéplacement (cu.in.)\n[, 4]\thp\tPuissance brute\n[, 5]\tmerde\tRapport de pont arrière\n[, 6]\tpoids\tPoids (1000 lb)\n[, 7]\tqsec\t1/4 de mile de temps\n[, 8]\tcontre\tMoteur (0 \u003d en forme de V, 1 \u003d droit)\n[, 9]\tsuis\tTransmission (0 \u003d automatique, 1 \u003d manuelle)\n[,dix]\tengrenage\tNombre de vitesses avant\n[,11]\tglucides\tNombre de carburateurs\n\nLa variable cible est la consommation en carburant des véhicules `mpg Miles/gallon` en fonction des variables explicatives."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql._            \nval spark \u003d SparkSession.builder().getOrCreate()\n\nval data \u003d spark.read.format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\")\n                .option(\"inferSchema\", \"true\") \n                .load(\"/opt/hupi/DATA/UPPA_2022/Data/regression/mtcars.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndata.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndata.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nz.show(data.describe())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Some descriptions of data\n#### Statistics summary "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n\n// Convert df to RDD to be able to use the library MultiVariateStatisticalSummary.\nval rdd \u003d data.drop(\"model\").map(l \u003d\u003e (l(0).asInstanceOf[Double], \n                                       l(1).asInstanceOf[Integer].toDouble, \n                                       l(2).asInstanceOf[Double],\n                                       l(3).asInstanceOf[Integer].toDouble,\n                                       l(4).asInstanceOf[Double],\n                                       l(5).asInstanceOf[Double],\n                                       l(6).asInstanceOf[Double],\n                                       l(7).asInstanceOf[Integer].toDouble, \n                                       l(8).asInstanceOf[Integer].toDouble,\n                                       l(9).asInstanceOf[Integer].toDouble,\n                                       l(10).asInstanceOf[Integer].toDouble)).rdd\n\n// Convert rdd to the rdd of vectors\nval observations \u003d rdd.map(l \u003d\u003e Vectors.dense(l._1, l._2, l._3, l._4, l._5))       "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary \u003d Statistics.colStats(observations)\nprintln(\"Vectors of observations\u0027 mean : \" + summary.mean)  \nprintln(\"Vectors of observations\u0027 variance : \" + summary.variance)  \nprintln(\"Vectors of observations\u0027 number of column not null : \" + summary.numNonzeros)  \nprintln()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.rdd.RDD\n\n// calculate the correlation matrix using Pearson\u0027s method. Use \"spearman\" for Spearman\u0027s method\n// If a method is not specified, Pearson\u0027s method will be used by default.\nval correlMatrix: Matrix \u003d Statistics.corr(observations, \"pearson\")\nprintln(correlMatrix.toString)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this example, we don\u0027t have many variables descriptives, so we suppose that we can use all variables to build the regression model. Otherwise, we need to do a selection of variables to select the variables that affect the most the target variable. To do selection variable, depending on the type of variables, we can use different methods. In Spark, we have some basic tools to do that, for example https://spark.apache.org/docs/latest/ml-features.html#feature-selectors.\n\n###  Vector Assembler\n\nTo prepare for the construction of linear regression by using ML library, we have to have a data with 2 columns only (\"label\" and \"features\"). To have that, we need to put all the variables descriptives into a single vector column named \"features\" and column of the target variable should be renamed to \"label\". "
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n\nval assembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\"))\n  .setOutputCol(\"features\")\n  \nval training \u003d assembler.transform(data)\n                        .select(\"mpg\", \"features\")\n                        .withColumnRenamed(\"mpg\", \"label\")\n\nval Array(train, test) \u003d data.randomSplit(Array(0.8, 0.2))                        "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Build a linear regression model \n\nTo have the best model, we can try to fluctuate the parameters such as : number of max iterations, regularization parameters, etc. To find all the parameters supported by Spark that we can play with, you can see it in : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.regression.LinearRegression\n\nval lr \u003d new LinearRegression()\n  .setMaxIter(10)\n  .setRegParam(0.3)\n  .setElasticNetParam(0.8)\n\n// Fit the model\nval lrModel \u003d lr.fit(training)\n\n// Print the coefficients and intercept for linear regression\nprintln(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Evaluation of model \n\nSome other metrics that can be computed : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionTrainingSummary"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Summarize the model over the training set and print out some metrics\nval trainingSummary \u003d lrModel.summary\nprintln(s\"numIterations: ${trainingSummary.totalIterations}\")\nprintln(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\ntrainingSummary.residuals.show()\nprintln(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\nprintln(s\"r2: ${trainingSummary.r2}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Conclusion\n\nWithout any optimization, the quality of the model is pretty good (r2 \u003d 0.76). In reality, we can try to optimize this indicator by removing the anomalies, selecting the most important features to train model, adding more observations or more variables and fluctuating the parameters when we train model...\n\n\n### Note :\n\nAll models created in Spark can be saved in HDFS by doing : \n\n* model.save(sc, \"file:///Apps/spark/data/mllib/testModelPath\") \n\nTo load it for future usage : \n\n* val sameModel \u003d SVMModel.load(sc, \"file:///Apps/spark/data/mllib/testModelPath\"). \n\nIn this example, it\u0027s SVM model, so it\u0027s SVMModel.load\n\nPlus, for some models, we can convert it to PMML format. It\u0027s good if you knew already PMML, if not, it\u0027s also fine ;) you can read here : https://www.ibm.com/developerworks/library/ba-ind-PMML1/index.html.\n\nYou can see list of supported models in Spark here : https://spark.apache.org/docs/2.0.0-preview/mllib-pmml-model-export.html"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Exercice\n## Comment peut-on transformer ce code en créant un pipeline ? \n## Comment peut-on améliorer le modèle avec une cross-validation ?\n\n### Pre-processing\nPréparer les phases de `assembler` et on va ajouter l\u0027étape de `Standardisation` des données.\nIl faut donc créer deux objets :\n- VectorAssembler\n- StandardScaler (https://spark.apache.org/docs/3.2.1/ml-features.html#standardscaler)\n\nRechercher dans la documentation les fonctions nécessaires : https://spark.apache.org/docs/3.2.1/ml-guide.html\n\nNous appelerons ces deux objets (ie. variables immuables \"val\") *assembler* et *scaler*. Ces deux premières étapes du pipeline, ont pour objectif de transformer et formater les données pour le modèle et de normaliser les données numériques, afin que les variables numériques soient comparables entre elles."
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval assembler \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval scaler \u003d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Model\nCréer le modèle de régression linéaire de votre choix (Linéaire simple, Lasso, Ridge, ElasticNet).\nCréer les ensembles de test et d\u0027entraînement."
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval lr \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval train, test \u003d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Pipeline\nCréer la chaîne pipeline avec les différentes étapes `stages`.\nPuis entraîner le modèle."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval pipeline \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval lrModel \u003d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Metrics\nCalculer les métriques des erreurs usuelles, qui sont le RMSE et le coefficient de détermination (noté r2).\nLe coefficient de détermination est une mesure de la qualité de la prédiction d\u0027une régression linéaire. Il représente le pourcentage de la variance expliquée par la régression (ie. des prédictions faites) sur la variance totale de la variable réelle. Cet indicateur estime donc la corrélation entre les prédictions et la réalité. Plus il est proche de 1, plus le modèle est performat.\n\n**! Attention :**\nIci il ne suffit pas de faire `lrModel.summary` car ici `lrModel` est de type pipeline. Il faut donc extraire d\u0027abord du pipeline la composante représentant le modèle.\nPour cela vous allez avoir besoin des objets suivants :\n- la fonction .stages(*numero_stage*) --\u003e en spécifiant le numéro de l\u0027étape qu\u0027on souhaite extraire\n- la fonction .asInstanceOf[*type_stage*] --\u003e en spécifiant le type de l\u0027objet qu\u0027on souhaite extraire du pipeline\n- la librairie du *type_stage* à extraire : import org.apache.spark.ml.regression.LinearRegressionModel\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval trainingSummary \u003d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test\nAppliquer le modèle entraîner sur l\u0027ensemble de données de test. Vous allez donc devoir utiliser la fonction `.transform`, qui permet d\u0027appliquer le modèle, avec les différentes étapes du pipeline qui sont nécessaires, automatiquement.\n\nEnsuite, évaluer les prédictions effectuées à l\u0027aide de l\u0027indicateur **RMSE**. Pour cela vous allez avoir besoin des objets suivants :\n- la librairie *org.apache.spark.ml*, contenant la fonction *evaluation.RegressionEvaluator* donc cela donne *org.apache.spark.ml.evaluation.RegressionEvaluator*\n- instancier un objet, qu\u0027on appelera *evaluator*, permettant de calculer le RMSE entre la variable réelle `Water` (en tant que LabelCol) et la variable prédite `prediction` (en tant que Predictionol).\n\nSuivre la documentation suivante : https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.ml.evaluation.RegressionEvaluator\n\nEnfin, appliquer l\u0027objet *evaluator* créé, à l\u0027objet *predictions* (dataframe contenant les prédictions faites sur l\u0027ensemble de test), en utilisant la fonction *.evaluate*.\nPuis pour finir afficher le résultat du RMSE."
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval predictions \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval evaluator \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rmse \u003d\nprintln(s\"Root Mean Squared Error (RMSE) on test data \u003d $rmse\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Cross validation\nL\u0027objectif est d\u0027utiliser la méthode de **Cross-validation** pour ajuster les différents paramètres de la régression linéaire.\nLes paramètres à ajuster sont les suivants :\n- regParam\n- elasticNetParam\n\nAfin de réaliser cela, nous allons tout d\u0027abord créer une grille de recherche.\nPar exemple, voici l\u0027ensemble des valeurs que nous allons tester pour chaque paramètre :\n- Pour `regParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.01, 0.2, 0.3\n- Pour `elasticNetParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.8\nUne fois que la grille est créée, il faut ensuite créer le modèle de cross-validation, puis pour terminer l\u0027appliquer à l\u0027ensemble d\u0027entraînement.\n\nEn étudiant la documentation construisez les objets suivants :\n- paramGrid\n- cv\n- cvModel\n\nEnfin tester à nouveau le nouveau modèle `cvModel` sur l\u0027ensemble de test."
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval paramGrid \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval cv \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval cvModel \u003d "
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval predictionsCvModel \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval evaluatorCvModel \u003d"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rmse \u003d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Correction\nà venir"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.LinearRegressionModel\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval assembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\"))\n  .setOutputCol(\"features\")\n\nval scaler \u003d new StandardScaler()\n  .setInputCol(\"features\")\n  .setOutputCol(\"scaledFeatures\")\n  .setWithStd(true)\n  .setWithMean(true)\n  \nval lr \u003d new LinearRegression()\n  .setMaxIter(10)\n  .setRegParam(0.3)\n  .setElasticNetParam(0.8)\n  .setLabelCol(\"mpg\")\n\nval Array(train, test) \u003d data.randomSplit(Array(0.8, 0.2)) \n\nval pipeline \u003d new Pipeline()\n  .setStages(Array(assembler, scaler, lr))\n  \nval lrModel \u003d pipeline.fit(train)\n\n// Summarize the model over the training set and print out some metrics\nprintln(s\"numIterations: ${trainingSummary.totalIterations}\")\nprintln(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\ntrainingSummary.residuals.show()\nprintln(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\nprintln(s\"r2: ${trainingSummary.r2}\")\n\n// Make predictions.\nval predictions \u003d lrModel.transform(test)\n\n// Select example rows to display.\npredictions.select(\"prediction\", \"mpg\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval evaluator \u003d new RegressionEvaluator()\n  .setLabelCol(\"mpg\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"rmse\")\n  \nval rmse \u003d evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data \u003d $rmse\")"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}