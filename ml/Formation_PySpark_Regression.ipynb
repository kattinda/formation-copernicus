{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bGlRFxyjIBnX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section, let us present to you some Machine Learning algorithms, there are many, but 3 algorithms below can be considered as the most popular in Machine Learning :\n",
        "\n",
        "- 1/ Regression - Linear Regression\n",
        "- 2/ Classification - Random Forest\n",
        "- 3/ Clustering - KMeans\n",
        "\n",
        "This notebook will focus on the first one, we'll take a dataset and then build a linear regression model based on it. \n",
        "\n",
        "\"Linear regression is the most basic type of regression and commonly used predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome variable?  Is the model using the predictors accounting for the variability in the changes in the dependent variable? (2) Which variables in particular are significant predictors of the dependent variable?  And in what way do they--indicated by the magnitude and sign of the beta estimates--impact the dependent variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. (3) What is the regression equation that shows how the set of predictor variables can be used to predict the outcome?  The simplest form of the equation with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent score, c = constant, b = regression coefficients, and x = independent variable.\"\n",
        "\n",
        "(source : http://www.statisticssolutions.com/what-is-linear-regression/)"
      ],
      "metadata": {
        "id": "0rIubc1KAs4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation et imports"
      ],
      "metadata": {
        "id": "60HJQGUBH8L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQq-tVEEa7nS",
        "outputId": "6bfbe45d-32f3-4a06-9a15-3defee1585a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=14194eb146feae8986f88d9e887bf07471c2f47f8a6712fced7f3c5342686c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-Mi4ymG-axew"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.stat import MultivariateStatisticalSummary, Statistics\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download datasets"
      ],
      "metadata": {
        "id": "bGlRFxyjIBnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1sp0G0pLgqn7hk7v6Sx2Il3Tg_nqZuSG4 -O dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0lGLrVTdXu4",
        "outputId": "5cbfce75-e044-46f4-edb8-320208195742"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sp0G0pLgqn7hk7v6Sx2Il3Tg_nqZuSG4\n",
            "To: /content/dataset.zip\n",
            "\r  0% 0.00/831k [00:00<?, ?B/s]\r100% 831k/831k [00:00<00:00, 104MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va6S1OK4iquU",
        "outputId": "2e5422eb-8339-4b01-a7f0-50c198b228bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/iris.csv        \n",
            "  inflating: __MACOSX/dataset/._iris.csv  \n",
            "  inflating: dataset/titanic.csv     \n",
            "  inflating: __MACOSX/dataset/._titanic.csv  \n",
            "  inflating: dataset/wine.csv        \n",
            "  inflating: __MACOSX/dataset/._wine.csv  \n",
            "  inflating: dataset/house.csv       \n",
            "  inflating: __MACOSX/dataset/._house.csv  \n",
            "  inflating: dataset/mtcars.csv      \n",
            "  inflating: __MACOSX/dataset/._mtcars.csv  \n",
            "  inflating: dataset/diabetes.csv    \n",
            "  inflating: __MACOSX/dataset/._diabetes.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls ./dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOkdS7qkjXya",
        "outputId": "746e8e48-b569-40e7-aba2-7ea1b7178028"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes.csv  house.csv  iris.csv  mtcars.csv  titanic.csv  wine.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modèle de régression avec Spark Scala\n",
        "\n",
        "Here we use the dataset from https://gist.github.com/seankross/a412dfbd88b3db70b74b#file-mtcars-csv\n",
        "\n",
        "The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n",
        "\n",
        "A data frame with 32 observations on 11 (numeric) variables.\n",
        "\n",
        "- mpg\t: Miles/(US) gallon\n",
        "- cyl :\tNumber of cylinders\n",
        "- disp : Displacement (cu.in.)\n",
        "- hp : Gross horsepower\n",
        "- drat : Rear axle ratio\n",
        "-\twt :\tWeight (1000 lbs)\n",
        "-\tqsec : 1/4 mile time\n",
        "- vs : Engine (0 = V-shaped, 1 = straight)\n",
        "- am : Transmission (0 = automatic, 1 = manual)\n",
        "- gear : Number of forward gears\n",
        "- carb : Number of carburetors\n",
        "\n",
        "La variable cible est la consommation en carburant des véhicules `mpg Miles/gallon` en fonction des variables explicatives."
      ],
      "metadata": {
        "id": "lihUQf8NRulK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession.builder\n",
        "                    .master(\"local[2]\")\n",
        "                    .appName(\"regression_model\")\n",
        "                    .getOrCreate())"
      ],
      "metadata": {
        "id": "HQMjymdTSkGX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read.format(\"com.databricks.spark.csv\")\n",
        "              .option(\"header\", \"true\")\n",
        "              .option(\"inferSchema\", \"true\") \n",
        "              .load(\"./dataset/mtcars.csv\"))"
      ],
      "metadata": {
        "id": "iX6FWyRTa24s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse des données\n",
        "\n",
        "1. Show the dataframe\n",
        "2. Print the schema of the dataframe\n",
        "3. Statistics summary \n",
        "4. Correlations of variables"
      ],
      "metadata": {
        "id": "msuSxs9Dg-jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "6oRHIAApgbEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570e852d-ad7e-47ba-ca0e-a184aab92090"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "jJbgTPjDgbBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964c54fb-7716-472a-86f2-648e29588685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- model: string (nullable = true)\n",
            " |-- mpg: double (nullable = true)\n",
            " |-- cyl: integer (nullable = true)\n",
            " |-- disp: double (nullable = true)\n",
            " |-- hp: integer (nullable = true)\n",
            " |-- drat: double (nullable = true)\n",
            " |-- wt: double (nullable = true)\n",
            " |-- qsec: double (nullable = true)\n",
            " |-- vs: integer (nullable = true)\n",
            " |-- am: integer (nullable = true)\n",
            " |-- gear: integer (nullable = true)\n",
            " |-- carb: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des données\n",
        "df.describe().show()"
      ],
      "metadata": {
        "id": "ETNZ6gOKga_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16ed616-48bb-4946-b1be-953bc5acff9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
            "|summary|      model|               mpg|               cyl|              disp|               hp|              drat|                wt|              qsec|                vs|                 am|              gear|              carb|\n",
            "+-------+-----------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
            "|  count|         32|                32|                32|                32|               32|                32|                32|                32|                32|                 32|                32|                32|\n",
            "|   mean|       null|20.090624999999996|            6.1875|230.72187500000004|         146.6875|3.5965625000000006|3.2172499999999995|17.848750000000003|            0.4375|            0.40625|            3.6875|            2.8125|\n",
            "| stddev|       null| 6.026948052089103|1.7859216469465444|123.93869383138195|68.56286848932059|0.5346787360709716|0.9784574429896968|1.7869432360968436|0.5040161287741853|0.49899091723584604|0.7378040652569471|1.6151999776318522|\n",
            "|    min|AMC Javelin|              10.4|                 4|              71.1|               52|              2.76|             1.513|              14.5|                 0|                  0|                 3|                 1|\n",
            "|    max| Volvo 142E|              33.9|                 8|             472.0|              335|              4.93|             5.424|              22.9|                 1|                  1|                 5|                 8|\n",
            "+-------+-----------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert df to RDD to be able to use the library MultiVariateStatisticalSummary.\n",
        "rdd = (df.drop(\"model\").map(l => (l(0).asInstanceOf[Double], \n",
        "                                  l(1).asInstanceOf[Integer].toDouble, \n",
        "                                  l(2).asInstanceOf[Double],\n",
        "                                  l(3).asInstanceOf[Integer].toDouble,\n",
        "                                  l(4).asInstanceOf[Double],\n",
        "                                  l(5).asInstanceOf[Double],\n",
        "                                  l(6).asInstanceOf[Double],\n",
        "                                  l(7).asInstanceOf[Integer].toDouble, \n",
        "                                  l(8).asInstanceOf[Integer].toDouble,\n",
        "                                  l(9).asInstanceOf[Integer].toDouble,\n",
        "                                  l(10).asInstanceOf[Integer].toDouble)).rdd)\n",
        "\n",
        "# Convert rdd to the rdd of vectors\n",
        "observations = rdd.map(l => Vectors.dense(l._1, l._2, l._3, l._4, l._5))   \n",
        "\n",
        "# Compute column summary statistics.\n",
        "summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n",
        "println(\"Vectors of observations' mean : \" + summary.mean)  \n",
        "println(\"Vectors of observations' variance : \" + summary.variance)  \n",
        "println(\"Vectors of observations' number of column not null : \" + summary.numNonzeros)  \n",
        "println()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "0Z9VQKzuBglr",
        "outputId": "51dfa4a3-bc4b-424e-bfa4-3937b9333e34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-07ae24da1f00>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    rdd = (df.drop(\"model\").map(l => (l(0).asInstanceOf[Double],\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n",
        "# If a method is not specified, Pearson's method will be used by default.\n",
        "correlMatrix = Statistics.corr(observations, \"pearson\")\n",
        "println(correlMatrix.toString)"
      ],
      "metadata": {
        "id": "l6HOlrIaCXYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modélisation\n",
        "\n",
        "In this example, we don't have many variables descriptives, so we suppose that we can use all variables to build the regression model. Otherwise, we need to do a selection of variables to select the variables that affect the most the target variable. To do selection variable, depending on the type of variables, we can use different methods. In Spark, we have some basic tools to do that, for example https://spark.apache.org/docs/latest/ml-features.html#feature-selectors.\n",
        "\n",
        "\n",
        "###  Vector Assembler\n",
        "\n",
        "To prepare for the construction of linear regression by using ML library, we have to have a data with 2 columns only (\"label\" and \"features\"). To have that, we need to put all the variables descriptives into a single vector column named \"features\" and column of the target variable should be renamed to \"label\". "
      ],
      "metadata": {
        "id": "DYRDaLccTchP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\"))\n",
        "  .setOutputCol(\"features\")\n",
        "  \n",
        "training = assembler.transform(data)\n",
        "                        .select(\"mpg\", \"features\")\n",
        "                        .withColumnRenamed(\"mpg\", \"label\")\n",
        "\n",
        "Array(train, test) = data.randomSplit(Array(0.8, 0.2))                        "
      ],
      "metadata": {
        "id": "JI8KtMkhhKMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a linear regression model \n",
        "\n",
        "To have the best model, we can try to fluctuate the parameters such as : number of max iterations, regularization parameters, etc. To find all the parameters supported by Spark that we can play with, you can see it in : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression"
      ],
      "metadata": {
        "id": "088RYcCrTzml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = new LinearRegression()\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "  .setElasticNetParam(0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for linear regression\n",
        "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
      ],
      "metadata": {
        "id": "V239NoqwjfTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of model \n",
        "\n",
        "Some other metrics that can be computed : https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionTrainingSummary"
      ],
      "metadata": {
        "id": "mm9pA9DdT4ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the model over the training set and print out some metrics\n",
        "trainingSummary = lrModel.summary\n",
        "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
        "println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n",
        "trainingSummary.residuals.show()\n",
        "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
        "println(s\"r2: ${trainingSummary.r2}\")"
      ],
      "metadata": {
        "id": "E3GOT4-vjfRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Without any optimization, the quality of the model is pretty good (r2 = 0.76). In reality, we can try to optimize this indicator by removing the anomalies, selecting the most important features to train model, adding more observations or more variables and fluctuating the parameters when we train model...\n",
        "\n",
        "### Note :\n",
        "\n",
        "All models created in Spark can be saved in HDFS by doing : \n",
        "\n",
        "* model.save(sc, \"file:///Apps/spark/data/mllib/testModelPath\") \n",
        "\n",
        "To load it for future usage : \n",
        "\n",
        "* val sameModel = SVMModel.load(sc, \"file:///Apps/spark/data/mllib/testModelPath\"). \n",
        "\n",
        "In this example, it's SVM model, so it's SVMModel.load\n",
        "\n",
        "Plus, for some models, we can convert it to PMML format. It's good if you knew already PMML, if not, it's also fine ;) you can read here : https://www.ibm.com/developerworks/library/ba-ind-PMML1/index.html.\n",
        "\n",
        "You can see list of supported models in Spark here : https://spark.apache.org/docs/2.0.0-preview/mllib-pmml-model-export.html"
      ],
      "metadata": {
        "id": "oA5iXb0DT9gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice\n",
        "## Comment peut-on transformer ce code en créant un pipeline ? \n",
        "## Comment peut-on améliorer le modèle avec une cross-validation ?\n",
        "\n",
        "\n",
        "### Pre-processing\n",
        "Préparer les phases de `assembler` et on va ajouter l'étape de `Standardisation` des données.\n",
        "Il faut donc créer deux objets :\n",
        "- VectorAssembler\n",
        "- StandardScaler (https://spark.apache.org/docs/3.2.1/ml-features.html#standardscaler)\n",
        "\n",
        "Rechercher dans la documentation les fonctions nécessaires : https://spark.apache.org/docs/3.2.1/ml-guide.html\n",
        "\n",
        "Nous appelerons ces deux objets (ie. variables immuables \"val\") *assembler* et *scaler*. Ces deux premières étapes du pipeline, ont pour objectif de transformer et formater les données pour le modèle et de normaliser les données numériques, afin que les variables numériques soient comparables entre elles."
      ],
      "metadata": {
        "id": "gZFM29B_UDkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler ="
      ],
      "metadata": {
        "id": "eSLk0ZACTbnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler ="
      ],
      "metadata": {
        "id": "Fv2jDoxiTble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "Créer le modèle de régression linéaire de votre choix (Linéaire simple, Lasso, Ridge, ElasticNet).\n",
        "Créer les ensembles de test et d'entraînement."
      ],
      "metadata": {
        "id": "z97tzmAQUIVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr ="
      ],
      "metadata": {
        "id": "AH2m9q1qTbjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test ="
      ],
      "metadata": {
        "id": "TmX3YZmATbgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline\n",
        "Créer la chaîne pipeline avec les différentes étapes `stages`.\n",
        "Puis entraîner le modèle."
      ],
      "metadata": {
        "id": "yrQzAbrLUL26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline ="
      ],
      "metadata": {
        "id": "iJrY4fU7TbeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrModel ="
      ],
      "metadata": {
        "id": "eujVtPgaUPfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics\n",
        "Calculer les métriques des erreurs usuelles, qui sont le RMSE et le coefficient de détermination (noté r2).\n",
        "Le coefficient de détermination est une mesure de la qualité de la prédiction d'une régression linéaire. Il représente le pourcentage de la variance expliquée par la régression (ie. des prédictions faites) sur la variance totale de la variable réelle. Cet indicateur estime donc la corrélation entre les prédictions et la réalité. Plus il est proche de 1, plus le modèle est performat.\n",
        "\n",
        "**! Attention :**\n",
        "Ici il ne suffit pas de faire `lrModel.summary` car ici `lrModel` est de type pipeline. Il faut donc extraire d'abord du pipeline la composante représentant le modèle.\n",
        "Pour cela vous allez avoir besoin des objets suivants :\n",
        "- la fonction .stages(*numero_stage*) --> en spécifiant le numéro de l'étape qu'on souhaite extraire\n",
        "- la fonction .asInstanceOf[*type_stage*] --> en spécifiant le type de l'objet qu'on souhaite extraire du pipeline\n",
        "- la librairie du *type_stage* à extraire : import org.apache.spark.ml.regression.LinearRegressionModel\n"
      ],
      "metadata": {
        "id": "Pf4zLIGTUNwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traininSummary ="
      ],
      "metadata": {
        "id": "6OCEcPB8jfMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "Appliquer le modèle entraîner sur l'ensemble de données de test. Vous allez donc devoir utiliser la fonction `.transform`, qui permet d'appliquer le modèle, avec les différentes étapes du pipeline qui sont nécessaires, automatiquement.\n",
        "\n",
        "Ensuite, évaluer les prédictions effectuées à l'aide de l'indicateur **RMSE**. Pour cela vous allez avoir besoin des objets suivants :\n",
        "- la librairie *org.apache.spark.ml*, contenant la fonction *evaluation.RegressionEvaluator* donc cela donne *org.apache.spark.ml.evaluation.RegressionEvaluator*\n",
        "- instancier un objet, qu'on appelera *evaluator*, permettant de calculer le RMSE entre la variable réelle `Water` (en tant que LabelCol) et la variable prédite `prediction` (en tant que Predictionol).\n",
        "\n",
        "Suivre la documentation suivante : https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.ml.evaluation.RegressionEvaluator\n",
        "\n",
        "Enfin, appliquer l'objet *evaluator* créé, à l'objet *predictions* (dataframe contenant les prédictions faites sur l'ensemble de test), en utilisant la fonction *.evaluate*.\n",
        "Puis pour finir afficher le résultat du RMSE."
      ],
      "metadata": {
        "id": "iRryuW13UTzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions ="
      ],
      "metadata": {
        "id": "1JzKWNHFjfKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator ="
      ],
      "metadata": {
        "id": "KV6GjJAUhKKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse =\n",
        "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
      ],
      "metadata": {
        "id": "H5baNSyLUThT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross validation\n",
        "L'objectif est d'utiliser la méthode de **Cross-validation** pour ajuster les différents paramètres de la régression linéaire.\n",
        "Les paramètres à ajuster sont les suivants :\n",
        "- regParam\n",
        "- elasticNetParam\n",
        "\n",
        "Afin de réaliser cela, nous allons tout d'abord créer une grille de recherche.\n",
        "Par exemple, voici l'ensemble des valeurs que nous allons tester pour chaque paramètre :\n",
        "- Pour `regParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.01, 0.2, 0.3\n",
        "- Pour `elasticNetParam` nous allos vouloir tester les valeurs suivantes : 0.1, 0.8\n",
        "Une fois que la grille est créée, il faut ensuite créer le modèle de cross-validation, puis pour terminer l'appliquer à l'ensemble d'entraînement.\n",
        "\n",
        "En étudiant la documentation construisez les objets suivants :\n",
        "- paramGrid\n",
        "- cv\n",
        "- cvModel\n",
        "\n",
        "Enfin tester à nouveau le nouveau modèle `cvModel` sur l'ensemble de test."
      ],
      "metadata": {
        "id": "2gefyPzSUYFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid ="
      ],
      "metadata": {
        "id": "I7icGXIoUTe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv ="
      ],
      "metadata": {
        "id": "vF__Vd9WUXrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel = "
      ],
      "metadata": {
        "id": "1wnTjLLKUXpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionsCvModel ="
      ],
      "metadata": {
        "id": "4wrYrfcIUXm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluatorCvModel ="
      ],
      "metadata": {
        "id": "4Sh3Nf1mUb1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse =\n",
        "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
      ],
      "metadata": {
        "id": "gmAdJ-2yUby7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correction\n",
        "à venir"
      ],
      "metadata": {
        "id": "aTzzfU70UeLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "scaler = new StandardScaler()\n",
        "  .setInputCol(\"features\")\n",
        "  .setOutputCol(\"scaledFeatures\")\n",
        "  .setWithStd(true)\n",
        "  .setWithMean(true)\n",
        "  \n",
        "lr = new LinearRegression()\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "  .setElasticNetParam(0.8)\n",
        "  .setLabelCol(\"mpg\")\n",
        "\n",
        "Array(train, test) = data.randomSplit(Array(0.8, 0.2)) \n",
        "\n",
        "pipeline = new Pipeline()\n",
        "  .setStages(Array(assembler, scaler, lr))\n",
        "  \n",
        "lrModel = pipeline.fit(train)\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n",
        "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
        "println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n",
        "trainingSummary.residuals.show()\n",
        "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
        "println(s\"r2: ${trainingSummary.r2}\")\n",
        "\n",
        "# Make predictions.\n",
        "predictions = lrModel.transform(test)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"mpg\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error.\n",
        "evaluator = new RegressionEvaluator()\n",
        "  .setLabelCol(\"mpg\")\n",
        "  .setPredictionCol(\"prediction\")\n",
        "  .setMetricName(\"rmse\")\n",
        "  \n",
        "rmse = evaluator.evaluate(predictions)\n",
        "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
      ],
      "metadata": {
        "id": "uCZg2TguUbwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soahZTiYUXkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uo8d_h-MhDEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbFP4K12hDCB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}